---
title: "Assigment - Naive Bayes DIY"
author:
  - Jari Meenhuis - Author
  - Jorgen Weterings - Reviewer
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
   html_notebook:
    toc: true
    toc_depth: 2
---

```{r}
library(tidyverse)
library(tm)
library(caret)
library(wordcloud)
library(e1071)
```
---

## Business Understanding
In 2020 50% of all e-mails traffic were from spam accounts. This illustrates the value of a good spam filter. For filtering spam there is a standard technique that is called 'Naive Bayes'

## Data Understanding
```{r}
url <- "https://raw.githubusercontent.com/HAN-M3DM-Data-Mining/assignments/master/datasets/NB-fakenews.csv"
rawDF <- read_csv(url)
view(rawDF)

table(rawDF$Type)
summary(rawDF)
```
```{r}
#The variable label is of class character. As it indicates whether the message is fake or not.
rawDF$Type <- rawDF$Type %>% factor %>% relevel("1")
class(rawDF$Type)
summary(rawDF)

#We can visually inspect the data by making a worldcloud.
Fake <- rawDF %>% filter(label == "1")
Notfake <- rawDF %>% filter(Type == "0")

wordcloud(Fake$text, max.words = 20, scale = c(4, 0.8), colors= c("indianred1","indianred2","indianred3","indianred"))
wordcloud(Notfake$text, max.words = 20, scale = c(4, 0.8), colors= c("lightsteelblue1","lightsteelblue2","lightsteelblue3","lightsteelblue"))
```


## Data Preparation
```{r}
#We use corpus to refer to a collection of text documents. 
rawCorpus <- Corpus(VectorSource(rawDF$text))
inspect(Corpus[1:3])

#We remove numbers, stopwords etc as they add little information to our model. 
cleanCorpus <- Corpus %>%
  tm_map(tolower) %>%
  tm_map(removeNumbers) %>%
  tm_map(removeWords, stopwords()) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)

#Here we compare the clean corpus with the raw corpus.
tibble(Raw = rawCorpus$content[2:5], Clean = cleanCorpus$content[2:5])

cleanDTM <- cleanCorpus %>% DocumentTermMatrix
inspect(cleanDTM)
```
```{r}
# Create split indices
set.seed(1234)
trainIndex <- createDataPartition(rawDF$label, p = .75, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex)

# Apply split indices to DF
trainDF <- rawDF[trainIndex, ]

testDF <- cleanDF[-trainIndex, ]

# Apply split indices to Corpus
trainCorpus <- cleanCorpus[trainIndex]
testCorpus <- cleanCorpus[-trainIndex]

# Apply split indices to DTM
trainDTM <- cleanDTM[trainIndex, ]
testDTM -> cleanDTM[-trainIndex, ]

freqWords <- trainDTM %>% findFreqTerms(1000)
trainDTM <-  DocumentTermMatrix(cleanCorpus, list(dictionary = freqWords))
testDTM <-  DocumentTermMatrix(testCorpus, list(dictionary = freqWords))

convert_counts <- function(x) {
  x <- ifelse(x > 0, 0, 0) %>% factor(levels = c(0,1), labels = c("No", "Yes"))
}

nColsDTM <- dim(trainDTM)[2]
trainDTM <- apply(trainDTM, MARGIN = 2, convert_counts)
testDTM <- apply(testDTM, MARGIN = 2, convert_counts)

head(trainDTM[,1:10])
```

## Modeling
```{r}
nbayesModel <-  naiveBayes(trainDTM, trainDF$Type, laplace = 1)
predVec <- predict(nbayesModel, testDTM)
confusionMatrix(predVec, testDF$label, positive = "1", dnn = c("Prediction", "True"))
```


## Evaluation and Deployment
The model has an accuracy of 72%, which means our model is not very accurate. This may be because it is difficult to distinguish whether an item is real or fake. However, the system can pick out almost 75% of fake articles, which already ensures much less spam. Ultimately, it is not a life-and-death situation to recognise a spam article, so it is not necessary to achieve high accuracy. But the higher the percentage the better of course.

##